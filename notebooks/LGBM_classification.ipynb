{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Returns with LightGBM for [M6 Competition](https://m6competition.com/)\n",
    "\n",
    "<br>__[Aleksei Mashlakov](https://aleksei-mashlakov.github.io/)__ \n",
    "<br>\n",
    "<br>\n",
    "<a href='https://www.buymeacoffee.com/amashlakov' target='_blank'><img height='50' style='border:0px;height:50px;' src='https://www.buymeacoffee.com/assets/img/guidelines/download-assets-2.svg' border='0' alt='Buy Me a Coffee' /></a>\n",
    "\n",
    "> :warning: **NO INVESTMENT ADVICE** :warning:â€‹ This notebook is for educational/informational purposes only. The author is not responsible for any losses incurred as a result of using this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import chart_studio.plotly as py\n",
    "import plotly.io as pio\n",
    "# pio.renderers.default = \"notebook\"\n",
    "pio.templates.default = \"simple_white\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# pd.options.display.float_format = '{:.4%}'.format\n",
    "\n",
    "# Reproducibility\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_path = Path.cwd().parent \n",
    "os.chdir(wd_path)\n",
    "data_path = wd_path / \"data\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m6 = pd.read_csv(data_path / \"template/M6_Universe.csv\", index_col=0)\n",
    "df_m6[\"symbol\"] = df_m6[\"symbol\"].str.replace(\"FB\", \"META\")\n",
    "df_m6.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = df_m6[df_m6[\"class\"]== \"Stock\"][\"symbol\"].to_list()\n",
    "etfs = df_m6[df_m6[\"class\"]== \"ETF\"][\"symbol\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE: int = 100 # number of stocks to sample\n",
    "FORECAST_HORIZON: int = 20 # days ahead to forecast\n",
    "PERIODS: int = 20 # number of periods in stock returns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from src.io import get_ticker_historical_data, get_dre_ticker_data, get_today_date\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "SAVE_DIRECTORY = data_path / \"results/tickers\"\n",
    "SAVE_TICKERS = False\n",
    "\n",
    "if not os.path.exists(SAVE_DIRECTORY):\n",
    "    os.makedirs(SAVE_DIRECTORY)\n",
    "\n",
    "tickers = df_m6[\"symbol\"].to_list()\n",
    "from_date = pd.to_datetime(\"2018-01-01\")\n",
    "to_date = pd.to_datetime(\"2023-02-03\")\n",
    "interval = \"1d\"\n",
    "tickers_data = dict()\n",
    "\n",
    "for ticker in tqdm(tickers):\n",
    "    try:\n",
    "        if ticker != \"DRE\":\n",
    "            data = get_ticker_historical_data(\n",
    "                ticker=ticker, from_date=from_date, to_date=to_date, interval=interval\n",
    "            )\n",
    "        else:\n",
    "            data = get_dre_ticker_data().loc[from_date:to_date, :]\n",
    "        # This returns a data frame of scraped stock data from yahoo\n",
    "        # data = pdr.DataReader(str(ticker), 'nasdaq', from_date, to_date)\n",
    "        tickers_data[ticker] = data\n",
    "        if SAVE_TICKERS:\n",
    "            data.reset_index().to_csv(\n",
    "                os.path.join(str(SAVE_DIRECTORY), f\"{ticker}_{interval}.csv\")\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Exception for {ticker=}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that DRE stock has been removed from yhoofinance API so we will use its historical data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read asset prices data (as provided by the M6 submission platform)\n",
    "\n",
    "path = data_path / \"raw\" / \"assets_m6.csv\"\n",
    "m6_price_data = pd.read_csv(path)\n",
    "m6_price_data[\"date\"] = pd.to_datetime(m6_price_data[\"date\"])\n",
    "m6_price_data[\"symbol\"] = m6_price_data[\"symbol\"].replace(\"FB\", \"META\")\n",
    "# m6_price_data = m6_price_data.pivot(index=\"date\", columns=\"symbol\", values=\"price\")\n",
    "m6_price_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.time_features import reindex_weekdays\n",
    "\n",
    "for k, df in tickers_data.items():\n",
    "    tickers_data[k] = reindex_weekdays(df, start_index=from_date, end_index=to_date)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert returns to ranking\n",
    "\n",
    ":warning: Note that this is not the best way to convert returns to ranking because it does not properly handle tight values. We will use this approach for simplicity :warning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ticker_features import calculate_pct_returns\n",
    "\n",
    "random_noise = np.random.normal(0, 1e-12, size=(100))\n",
    "df = (\n",
    "    pd.DataFrame.from_dict({k: v[\"Adj Close\"] for k, v in tickers_data.items()})\n",
    "    .apply(calculate_pct_returns, periods=PERIODS, axis=0)\n",
    "    .dropna()\n",
    "    .apply(lambda x: x + random_noise, axis=1)\n",
    ")\n",
    "\n",
    "target_data = df.copy()\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    target_data.loc[idx, :] = (pd.qcut(row, q=[0, 0.2, 0.4, 0.6, 0.8, 1]).astype('category').cat.codes).values\n",
    "    if target_data.loc[idx, :].sum() !=200:\n",
    "        print(idx, target_data.loc[idx, :].sum())\n",
    "\n",
    "target_data = target_data.astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way to get ranking would be the following:\n",
    "```python\n",
    "from src.ticker_features import calculate_pct_returns\n",
    "\n",
    "df = pd.DataFrame.from_dict({k: v[\"Adj Close\"] for k, v in tickers_data.items()})\n",
    "\n",
    "target_data2 = (\n",
    "    (\n",
    "        df.copy()\n",
    "        .apply(calculate_pct_returns, periods=PERIODS, axis=0)\n",
    "        .apply(lambda x: x + random_noise, axis=1)\n",
    "        .dropna()\n",
    "        .rank(1, ascending=True, method=\"min\")\n",
    "        // (20.0 + 1e-12)\n",
    "        + 1\n",
    "    )\n",
    "    .clip(upper=5)\n",
    "    .astype(int)\n",
    ")\n",
    "target_data2 -= 1\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main pipeline\n",
    "\n",
    "* Uses custom trading strategy with several indexes (SMA, EMA, RSI, MACD, etc.)\n",
    "* Features based on the transformation of stock prices, volumes\n",
    "* Categorical features based on the ticker properties (sector, industry, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from src.reduce_memory import ReduceMemoryTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from src.strategy import CustomStrategy1, CustomStrategy\n",
    "from src.ticker_features import upper_shadow, lower_shadow, upper_shadow_percent, lower_shadow_percent\n",
    "from src.transformers import DateTimeTransformer\n",
    "from src.ticker_features import calculate_pct_returns, calculate_log_returns, calculate_cum_log_returns, calculate_cum_pct_returns\n",
    "\n",
    "tickers_data_enriched = {}\n",
    "\n",
    "date_time_transforms = make_pipeline(\n",
    "    DateTimeTransformer()\n",
    ")\n",
    "\n",
    "memory_transforms = make_pipeline(\n",
    "    ReduceMemoryTransformer()\n",
    ")\n",
    "\n",
    "for k, v in tickers_data.items():\n",
    "    df = v.copy()\n",
    "    df = reindex_weekdays(df, drop_weekends=True, start_index=pd.to_datetime(\"2018-01-01\"))\n",
    "    df.ta.strategy(CustomStrategy)\n",
    "    # df.ta.percent_return(cumulative=False, append=True)\n",
    "    # df.ta.percent_return(cumulative=False, length=PERIODS, append=True)\n",
    "    df[f\"cum_log_returns_{PERIODS}\"] = df[[\"Adj Close\"]].apply(calculate_cum_log_returns, periods=PERIODS, axis=0).values\n",
    "    df[f\"log_returns_{PERIODS}\"] = df[[\"Adj Close\"]].apply(calculate_log_returns, periods=PERIODS, axis=0).values\n",
    "    df[f\"cum_prc_returns_{PERIODS}\"] = df[[\"Adj Close\"]].apply(calculate_cum_pct_returns, periods=PERIODS, axis=0).values\n",
    "    df[f\"prc_returns_{PERIODS}\"] = df[[\"Adj Close\"]].apply(calculate_pct_returns, periods=PERIODS, axis=0).values\n",
    "    df['high2low'] = df['High'] / df['Low']\n",
    "    df['high_low'] = df['High'] - df['Low']\n",
    "    df[f'var_{PERIODS}'] = df['Adj Close'].rolling(20).var()\n",
    "    # df['target_var'] = df[f'PCTRET_{PERIODS}'].var()\n",
    "    df['upper_shadow'] = upper_shadow(df)\n",
    "    df['lower_shadow'] = lower_shadow(df)\n",
    "    df['upper_shadow_percent'] = upper_shadow_percent(df)\n",
    "    df['lower_shadow_percent'] = lower_shadow_percent(df)    \n",
    "    df[\"log_volume\"] = np.log(df[\"Volume\"] + 1e-8)\n",
    "    df[\"log_high\"] = np.log(df[\"High\"] + 1e-8)\n",
    "    df[\"log_low\"] = np.log(df[\"Low\"] + 1e-8)\n",
    "    df = df.fillna(method=\"ffill\")\n",
    "    df = reindex_weekdays(df, drop_weekends=True, start_index=pd.to_datetime(\"2018-01-01\"))\n",
    "    df = memory_transforms.fit_transform(df)\n",
    "    \n",
    "    df[\"GICS_sector/ETF_type\"] = df_m6[df_m6[\"symbol\"]==k][\"GICS_sector/ETF_type\"].values[0]\n",
    "    df[\"GICS_industry/ETF_subtype\"] = df_m6[df_m6[\"symbol\"]==k][\"GICS_industry/ETF_subtype\"].values[0]\n",
    "    df[\"group\"] = k\n",
    "    df[\"ticker\"] = \"stock\" if k in stocks else \"etf\"\n",
    "    #df[\"month\"] = df.index.month.astype(str).astype(\"category\")  # categories have be strings\n",
    "    #df[\"day_of_week\"] = df.index.day_of_week #.astype(str).astype(\"category\")  # categories have be strings\n",
    "    #     scaler = MinMaxScaler() #StandardScaler()\n",
    "    #     df_scaled = pd.DataFrame(data=scaler.fit_transform(df), \n",
    "    #                              index=df.index,\n",
    "    #                              columns=df.columns)\n",
    "    #     df_scaled.dropna(inplace=True)\n",
    "    #     tickers_data_enriched[k] = df_scaled\n",
    "    tickers_data_enriched[k] = df#[df_stock_returns_quantiles.index[0]:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add datetime features \n",
    "\n",
    "* One hot encodings of holidays\n",
    "* Spline of day of the week\n",
    "* Kernels for holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.time_features import get_datetime_covariates\n",
    "\n",
    "covariates = get_datetime_covariates(from_date, to_date, memory_transforms=memory_transforms, date_time_transforms=date_time_transforms)\n",
    "covariates = reindex_weekdays(covariates, drop_weekends=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([pd.concat(\n",
    "[target_data[[k]].astype(int).rename(columns={k: \"target\"}),\n",
    "    target_data[[k]].astype(int).rename(columns={k: \"last_target\"}).shift(PERIODS),\n",
    "    tickers_data_enriched[k].shift(PERIODS), \n",
    "    covariates.shift(PERIODS)\n",
    "    ], axis=1).dropna(how=\"all\", axis=0)\n",
    "                for k in tickers_data_enriched.keys()])\n",
    "\n",
    "data = data.dropna()\n",
    "data[\"target\"] = data[\"target\"].astype(int)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_varying_known_categoricals = ['day_of_week', 'month'] # , \n",
    "static_columns = [\"group\", \"ticker\", 'GICS_sector/ETF_type','GICS_industry/ETF_subtype']#, \"month\"]\n",
    "time_var_reals = list(data.columns[(~data.columns.isin(static_columns+[\"target\"]))])\n",
    "\n",
    "# data = memory_transforms.fit_transform(data)\n",
    "data['target'] = data['target'].astype(int)\n",
    "# data['day_of_week'] = data['day_of_week'].astype(str)\n",
    "# data['month'] = data['month'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "for column in static_columns:\n",
    "    labels = data[column].unique()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(labels)\n",
    "    data[column] = le.transform(data[column].values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The data is fully ready now, lets do the backtesting and evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, lets test the LightGBM model with calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from src.metrics import portfolio_rps\n",
    "\n",
    "train_start = data.index[0]\n",
    "scalers = []\n",
    "classifiers = []\n",
    "train_rpss = []\n",
    "test_rpss = []\n",
    "\n",
    "for i, test_start in enumerate(\n",
    "    pd.date_range(start=\"2022-03-04\", end=\"2023-02-03\", freq=\"28D\", inclusive=\"left\")\n",
    "):\n",
    "    test_end = test_start + pd.Timedelta(days=28)\n",
    "    print(f\"Competition Month {i+1:2d}\")\n",
    "    # use the last tree months of training data for calibration\n",
    "    valid_cutoff = test_start - pd.Timedelta(days=28 * 3)\n",
    "    # calib_cutoff = test_start - pd.Timedelta(days=1)\n",
    "    print(f\"Train data: {train_start.date()} -- {valid_cutoff.date()- pd.Timedelta(days=1)}\")\n",
    "    print(f\"Valid data: {valid_cutoff.date()} -- {test_start.date()}\")\n",
    "    print(f\"Test data: {test_end.date()} -- {test_end.date()}\")\n",
    "\n",
    "    # split the data per group into train, calibration and test\n",
    "    groupby_full = data.groupby(\"group\")\n",
    "    groupby_train = groupby_full.apply(lambda group: group.loc[:(valid_cutoff - pd.Timedelta(days=1))])\n",
    "    groupby_valid = groupby_full.apply(lambda group: group.loc[valid_cutoff:test_start])\n",
    "    groupby_test = groupby_full.apply(lambda group: group.loc[test_end:test_end])\n",
    "\n",
    "    X_scaler = MinMaxScaler()\n",
    "\n",
    "    groupby_train.loc[:, time_var_reals] = X_scaler.fit_transform(\n",
    "        groupby_train.loc[:, time_var_reals]\n",
    "    )\n",
    "    y_train_df = groupby_train.loc[:, \"target\"]\n",
    "    X_train_df = groupby_train.drop([\"target\"], axis=1)\n",
    "\n",
    "    groupby_valid.loc[:, time_var_reals] = X_scaler.transform(\n",
    "        groupby_valid.loc[:, time_var_reals]\n",
    "    )\n",
    "    y_calib_df = groupby_valid.loc[:, \"target\"]\n",
    "    X_calib_df = groupby_valid.drop([\"target\"], axis=1)\n",
    "\n",
    "    groupby_test.loc[:, time_var_reals] = X_scaler.transform(\n",
    "        groupby_test.loc[:, time_var_reals]\n",
    "    )\n",
    "    y_test_df = groupby_test.loc[:, \"target\"]\n",
    "    X_test_df = groupby_test.drop([\"target\"], axis=1)\n",
    "\n",
    "    scalers.append(X_scaler)\n",
    "\n",
    "    # groupby_submit.loc[:, time_var_reals] = X_scaler.transform(groupby_submit.loc[:, time_var_reals])\n",
    "\n",
    "    X_train = X_train_df.values\n",
    "    y_train = y_train_df.values.astype(int)\n",
    "\n",
    "    X_calib = X_calib_df.values\n",
    "    y_calib = y_calib_df.values.astype(int)\n",
    "\n",
    "    X_test = X_test_df.values\n",
    "    y_test = y_test_df.values.astype(int)\n",
    "\n",
    "    # X_train, X_calib, y_train, y_calib = train_test_split(X, y, random_state=42, shuffle=False)\n",
    "    # concatenate train and calibration data for classifier without calibration\n",
    "    x_tr = np.concatenate((X_train, X_calib), axis=0)\n",
    "    y_tr = np.concatenate((y_train, y_calib), axis=0)\n",
    "\n",
    "    clf = LGBMClassifier(\n",
    "            boosting_type=\"gbdt\",\n",
    "            learning_rate=0.02,\n",
    "            n_estimators=100,\n",
    "            random_state=10,\n",
    "            max_depth=15,\n",
    "            importance_type='gain'\n",
    "            #lambda_l2=1.0,\n",
    "    )  # min_data_in_leaf=500, num_leaves=50, max_depth=10,\n",
    "\n",
    "    ### Model 1 - No calibration\n",
    "\n",
    "    clf.fit(x_tr, y_tr)\n",
    "    cal_clf_cv_probs_train = clf.predict_proba(x_tr)\n",
    "    cal_clf_cv_probs_test = clf.predict_proba(X_test)\n",
    "\n",
    "    n_values = np.max(y_tr) + 1\n",
    "    targets_train = np.eye(n_values)[y_tr]\n",
    "\n",
    "    n_values = np.max(y_test) + 1\n",
    "    targets_test = np.eye(n_values)[y_test]\n",
    "\n",
    "    ### Model 2 - Isotonic calibration\n",
    "\n",
    "    # clf.fit(X_train, y_train)\n",
    "    # cal_clf = CalibratedClassifierCV(clf, method=\"isotonic\", cv=\"prefit\", ensemble=True)\n",
    "    # cal_clf.fit(X_calib, y_calib)\n",
    "    # cal_clf_cv_probs_train = cal_clf.predict_proba(X_train)\n",
    "    # cal_clf_cv_probs_test = cal_clf.predict_proba(X_test)\n",
    "\n",
    "    # n_values = np.max(y_train)+1\n",
    "    # targets_train = np.eye(n_values)[y_train]\n",
    "\n",
    "    # n_values = np.max(y_test)+1\n",
    "    # targets_test = np.eye(n_values)[y_test]\n",
    "\n",
    "    ### Model 3 - Isotonic calibration with cross validation\n",
    "\n",
    "    # cal_clf_cv = CalibratedClassifierCV(\n",
    "    #     clf, method=\"isotonic\", cv=TimeSeriesSplit(n_splits=5), ensemble=True\n",
    "    # )\n",
    "\n",
    "    # cal_clf_cv.fit(x_tr, y_tr)\n",
    "    # cal_clf_cv_probs_train = cal_clf_cv.predict_proba(x_tr)\n",
    "    # cal_clf_cv_probs_test = cal_clf_cv.predict_proba(X_test)\n",
    "\n",
    "    # n_values = np.max(y_tr) + 1\n",
    "    # targets_train = np.eye(n_values)[y_tr]\n",
    "\n",
    "    # n_values = np.max(y_test) + 1\n",
    "    # targets_test = np.eye(n_values)[y_test]\n",
    "\n",
    "    train_rps = portfolio_rps(probs=cal_clf_cv_probs_train, outcome=targets_train)\n",
    "    test_rps = portfolio_rps(probs=cal_clf_cv_probs_test, outcome=targets_test)\n",
    "\n",
    "    train_rpss.append(train_rps)\n",
    "    test_rpss.append(test_rps)\n",
    "\n",
    "    print(f\"Month {i+1:2d}: {test_start.date()} -- {test_end.date()}\")\n",
    "    # print(f\"Classifier: {clf}\")\n",
    "    print(f\"Month {i+1:2d}: {train_rps=}\")\n",
    "    print(f\"Month {i+1:2d}: {test_rps=}\")\n",
    "    print(\"  ----------------------------------------  \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, X_train_df, importance_type):\n",
    "    # based on https://www.kaggle.com/code/skylord/lgb-model-feature-importance/notebook\n",
    "    model.importance_type = importance_type\n",
    "    feature_imp = pd.DataFrame(\n",
    "        zip(model.feature_importances_, list(X_train_df.columns)),\n",
    "        columns=[f\"Value_{clf.importance_type.capitalize()}\", \"Feature\"],\n",
    "    )\n",
    "    feature_imp.sort_values(\n",
    "        by=[f\"Value_{clf.importance_type.capitalize()}\"], ascending=True, inplace=True\n",
    "    )\n",
    "\n",
    "    trace2 = go.Bar(\n",
    "        y=feature_imp[\"Feature\"],\n",
    "        x=feature_imp[f\"Value_{clf.importance_type.capitalize()}\"],\n",
    "        name=f\"feature_importance_{clf.importance_type}\",\n",
    "        marker=dict(\n",
    "            color=\"rgba(174, 255, 255, 0.5)\", line=dict(color=\"rgb(0,0,0)\", width=1.5)\n",
    "        ),\n",
    "        orientation=\"h\",\n",
    "        text=feature_imp[\"Feature\"],\n",
    "    )\n",
    "\n",
    "    fig_data = [trace2]\n",
    "    layout = go.Layout(barmode=\"group\", title=f\"Feature Importance by {clf.importance_type.capitalize()}\")\n",
    "    fig = go.Figure(data=fig_data, layout=layout)\n",
    "    fig.write_html(data_path / f\"results/lgbm_feature_importance_by_{clf.importance_type}.html\")\n",
    "    py.plot(fig, filename=f\"lgbm_feature_importance_by_{clf.importance_type}\", auto_open = True)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_feature_importance(clf, X_train_df.rename(columns={\"ticket\":\"ticker\"}), importance_type='split')\n",
    "plot_feature_importance(clf, X_train_df.rename(columns={\"ticket\":\"ticker\"}), importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rps_backtest = pd.DataFrame(data={\"Train RPS\": train_rpss, \"Test RPS\": test_rpss}, index=[i for i in range(1, 13)])\n",
    "rps_backtest.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = rps_backtest[\"Train RPS\"].values\n",
    "train_quarter_rps = [np.mean(returns[i : i + 3]) for i in range(0, 12, 3) if i < len(returns)]\n",
    "returns = rps_backtest[\"Test RPS\"].values\n",
    "test_quarter_rps = [np.mean(returns[i : i + 3]) for i in range(0, 12, 3) if i < len(returns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "rps_backtest_quarter = pd.DataFrame(data={\"Train RPS\": train_quarter_rps, \"Test RPS\": test_quarter_rps}, index=[i for i in range(1, 5)])\n",
    "fig = px.bar(rps_backtest_quarter, x=rps_backtest_quarter.index, \n",
    "             y=[\"Test RPS\", \"Train RPS\"], barmode=\"group\", color=\"variable\", title=\"RPS quaterly backtest\")\n",
    "fig.add_hline(y=rps_backtest[\"Test RPS\"].mean(), line_dash=\"dash\", line_width=3, line_color=\"blue\")\n",
    "fig.add_hline(y=rps_backtest[\"Train RPS\"].mean(), line_dash=\"dash\", line_width=3, line_color=\"orange\")\n",
    "fig.add_hline(y=0.16, line_dash=\"dash\", line_width=3, line_color=\"red\", name=\"Benchmark\")\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    width=600,\n",
    "    xaxis_title=\"Quarter\",\n",
    "    yaxis_title=\"RPS\",\n",
    ")\n",
    "fig.write_html(data_path / \"results/rps_quarterly_backtest.html\")\n",
    "py.plot(fig, filename=f\"rps_quarterly_backtest\", auto_open = True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "rps_backtest = pd.DataFrame(data={\"Train RPS\": train_rpss, \"Test RPS\": test_rpss}, index=[i for i in range(1, 13)])\n",
    "fig = px.bar(rps_backtest, x=rps_backtest.index, y=[\"Test RPS\", \"Train RPS\"], \n",
    "             barmode=\"group\", color=\"variable\", title=\"RPS monthly backtest\")\n",
    "fig.add_hline(y=rps_backtest[\"Test RPS\"].mean(), line_dash=\"dash\", line_width=3, line_color=\"blue\")\n",
    "fig.add_hline(y=rps_backtest[\"Train RPS\"].mean(), line_dash=\"dash\", line_width=3, line_color=\"orange\")\n",
    "fig.add_hline(y=0.16, line_dash=\"dash\", line_width=3, line_color=\"red\", name=\"Benchmark\")\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    width=700,\n",
    "    xaxis_title=\"Month\",\n",
    "    yaxis_title=\"RPS\",\n",
    ")\n",
    "fig.write_html(data_path / \"results/rps_monthly_backtest.html\")\n",
    "py.plot(fig, filename=f\"rps_monthly_backtest\", auto_open = True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will track how many training rounds we needed for our best score.\n",
    "# We will use that number of rounds later.\n",
    "best_score = 999\n",
    "training_rounds = 10000\n",
    "from src.darts_hop import logging_callback\n",
    "import optuna\n",
    "\n",
    "# Declare how we evaluate how good a set of hyperparameters are, i.e.\n",
    "# declare an objective function.\n",
    "def objective(trial):\n",
    "    # Specify a search space using distributions across plausible values of hyperparameters.\n",
    "    param = {\n",
    "        #\"objective\": \"binary\",\n",
    "        #\"metric\": \"binary_error\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\", #trial.suggest_categorical(\"boosting_type\", choices=[\"gbdt\", \"dart\", \"rf\"]),                \n",
    "        \"seed\": 42,\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "    \n",
    "    # Run LightGBM for the hyperparameter values\n",
    "    clf = LGBMClassifier(**param,)\n",
    "    cal_clf_cv = CalibratedClassifierCV(clf, method=\"isotonic\", cv=TimeSeriesSplit(n_splits=5), ensemble=True)\n",
    "\n",
    "    cal_clf_cv.fit(x_tr, y_tr)\n",
    "    cal_clf_cv_probs_train = cal_clf_cv.predict_proba(x_tr)\n",
    "    cal_clf_cv_probs_test = cal_clf_cv.predict_proba(X_test)\n",
    "\n",
    "    n_values = np.max(y_tr)+1\n",
    "    targets_train = np.eye(n_values)[y_tr]\n",
    "\n",
    "    n_values = np.max(y_test)+1\n",
    "    targets_test = np.eye(n_values)[y_test]\n",
    "    \n",
    "\n",
    "    print(f\"Classifier: {clf}\")\n",
    "    train_rps = portfolio_rps(probs=cal_clf_cv_probs_train, outcome=targets_train)\n",
    "    test_rps = portfolio_rps(probs=cal_clf_cv_probs_test, outcome=targets_test)\n",
    "    # print(f\"cal_clf test RPS = {portfolio_rps(probs=cal_clf_probs, outcome=targets_test)}\")\n",
    "    print(f\"train RPS = {train_rps}\")\n",
    "    print(f\"test RPS = {test_rps}\")\n",
    "    \n",
    "    # Return metric of interest\n",
    "    return test_rps\n",
    "\n",
    "# Suppress information only outputs - otherwise optuna is \n",
    "# quite verbose, which can be nice, but takes up a lot of space\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
    "\n",
    "# We search for another 4 hours (3600 s are an hours, so timeout=14400).\n",
    "# We could instead do e.g. n_trials=1000, to try 1000 hyperparameters chosen \n",
    "# by optuna or set neither timeout or n_trials so that we keep going until \n",
    "# the user interrupts (\"Cancel run\").\n",
    "study = optuna.create_study(direction='minimize',  sampler=optuna.samplers.TPESampler(seed=42))  \n",
    "study.optimize(objective, n_trials=100, timeout=14400, callbacks=[logging_callback])\n",
    "\n",
    "print(f\"Best trial: \\n{study.best_trial}\\n\")\n",
    "print(f\"Best value: {study.best_value}\\n\")\n",
    "print(f\"Best params: {study.best_params}\\n\")\n",
    "# print(study.trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for submission\n",
    "clf = LGBMClassifier(boosting_type='gbdt', learning_rate=0.02, n_estimators=100, random_state=10, max_depth=15)\n",
    "X_tr = np.concatenate((X_train, X_calib, X_test), axis=0)\n",
    "y_tr = np.concatenate((y_train, y_calib, y_test), axis=0)\n",
    "cal_clf = CalibratedClassifierCV(clf, method=\"isotonic\", cv=5, ensemble=True)\n",
    "cal_clf.fit(X_tr, y_tr)\n",
    "preds = cal_clf.predict_proba(groupby_submit.values)\n",
    "preds = pd.DataFrame(data=preds, index=les[0].inverse_transform(groupby_submit.group.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_forecasts(month: int, preds: pd.DataFrame):\n",
    "    decimals = 5\n",
    "    df_submission = pd.read_csv(data_path / \"template/template.csv\")\n",
    "    df_submission[\"ID\"] = df_submission[\"ID\"].replace(\"FB\", \"META\")\n",
    "    preds = preds.reindex(index=df_submission.ID.values)\n",
    "    df_submission.set_index(\"ID\", inplace=True)\n",
    "    df_submission.iloc[:,:-1] = preds.values\n",
    "    df_submission.iloc[:,:-1] = df_submission.iloc[:,:-1].round(decimals)\n",
    "    df_submission.iloc[:, 0] += (1 - df_submission.iloc[:,:-1].sum(axis=1))\n",
    "    df_submission.round(decimals).to_csv( data_path / f\"results/lgbm_submission_{month}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will track how many training rounds we needed for our best score.\n",
    "# We will use that number of rounds later.\n",
    "best_score = 999\n",
    "training_rounds = 10000\n",
    "from src.darts_hop import logging_callback\n",
    "import optuna\n",
    "\n",
    "# Declare how we evaluate how good a set of hyperparameters are, i.e.\n",
    "# declare an objective function.\n",
    "def objective(trial):\n",
    "    # Specify a search space using distributions across plausible values of hyperparameters.\n",
    "    param = {\n",
    "        #\"objective\": \"binary\",\n",
    "        #\"metric\": \"binary_error\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\", #trial.suggest_categorical(\"boosting_type\", choices=[\"gbdt\", \"dart\", \"rf\"]),                \n",
    "        \"seed\": 42,\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "    \n",
    "    # Run LightGBM for the hyperparameter values\n",
    "    clf = LGBMClassifier(**param,)\n",
    "    cal_clf_cv = CalibratedClassifierCV(clf, method=\"isotonic\", cv=TimeSeriesSplit(n_splits=5), ensemble=True)\n",
    "\n",
    "    cal_clf_cv.fit(x_tr, y_tr)\n",
    "    cal_clf_cv_probs_train = cal_clf_cv.predict_proba(x_tr)\n",
    "    cal_clf_cv_probs_test = cal_clf_cv.predict_proba(X_test)\n",
    "\n",
    "    n_values = np.max(y_tr)+1\n",
    "    targets_train = np.eye(n_values)[y_tr]\n",
    "\n",
    "    n_values = np.max(y_test)+1\n",
    "    targets_test = np.eye(n_values)[y_test]\n",
    "    \n",
    "\n",
    "    print(f\"Classifier: {clf}\")\n",
    "    train_rps = portfolio_rps(probs=cal_clf_cv_probs_train, outcome=targets_train)\n",
    "    test_rps = portfolio_rps(probs=cal_clf_cv_probs_test, outcome=targets_test)\n",
    "    # print(f\"cal_clf test RPS = {portfolio_rps(probs=cal_clf_probs, outcome=targets_test)}\")\n",
    "    print(f\"train RPS = {train_rps}\")\n",
    "    print(f\"test RPS = {test_rps}\")\n",
    "    \n",
    "    # Return metric of interest\n",
    "    return test_rps\n",
    "\n",
    "# Suppress information only outputs - otherwise optuna is \n",
    "# quite verbose, which can be nice, but takes up a lot of space\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
    "\n",
    "# We search for another 4 hours (3600 s are an hours, so timeout=14400).\n",
    "# We could instead do e.g. n_trials=1000, to try 1000 hyperparameters chosen \n",
    "# by optuna or set neither timeout or n_trials so that we keep going until \n",
    "# the user interrupts (\"Cancel run\").\n",
    "study = optuna.create_study(direction='minimize',  sampler=optuna.samplers.TPESampler(seed=42))  \n",
    "study.optimize(objective, n_trials=100, timeout=14400, callbacks=[logging_callback])\n",
    "\n",
    "print(f\"Best trial: \\n{study.best_trial}\\n\")\n",
    "print(f\"Best value: {study.best_value}\\n\")\n",
    "print(f\"Best params: {study.best_params}\\n\")\n",
    "# print(study.trials)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the other classifiers (without tuning they performed worse than LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sktime.transformations.panel import rocket\n",
    "from sktime.classification.hybrid import HIVECOTEV2\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    HistGradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(10),\n",
    "    RocketClassifier(\n",
    "        num_kernels=100,\n",
    "    ),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF([1.0, 1.0])),\n",
    "    LinearSVC(random_state=10, max_iter=500),\n",
    "    RidgeClassifier(),\n",
    "    GaussianNB(),\n",
    "    HistGradientBoostingClassifier(\n",
    "        learning_rate=0.1,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        max_iter=1000,\n",
    "        l2_regularization=0.0,\n",
    "        max_depth=50,\n",
    "        random_state=10,\n",
    "    ),\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=5, max_features=\"auto\", random_state=10\n",
    "    ),  # max_features=1, min_samples_leaf=1,\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=10),\n",
    "    LogisticRegression(C=10.0, max_iter=100),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8e0bb0da2aff65736b499a73199d9b3916fe5784b22bc0d777fb56d771df7b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
